# -*- coding: utf-8 -*-
"""Copy of extremeDDF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OZagWmAfklCSWjWriaSA9Dl8mIbgxZ4M

# A Beginnerâ€™s Guide to Carry out Extreme Value Analysis (4) - DDF

Intensity Duration Frequency (IDF) or Depth Duration Frequency (DDF) curves are among the most common tools used in water resources management. They are derived from historical rainfall records under the assumption of stationarity. Rainfall IDF/DDF curve information describes the frequency of extreme rainfall events for a variety of durations and intensities.

The following presents a primary procedure to create DDF curves.

## 1. Load precipitation data
This is a hourly data precipitation data. The data have been convert to annual maxima time series for the durations of 1h, 3h, 6h, 12h, and 24h.
"""
import matplotlib
# matplotlib.use('agg')
from matplotlib import pyplot as plt
from matplotlib.pylab import rcParams
import os
import pandas as pd
from datetime import datetime
import lmoments3 as lmoments
from lmoments3 import distr
from statsmodels.distributions.empirical_distribution import ECDF
from collections import OrderedDict
import numpy as np


# set some rc-params for the session...
# rcParams['figure.figsize'] = 15, 9

# open some data
data = pd.read_csv('/Users/malindgren/Documents/TEMP/test_durations_ams_Fairbanks.csv', index_col=0)

# In the data, the 60m, 3h, 6h, 12h and 24h are durations, which is also the accumulation time for precipitation.
# simple check for each duration

ax = data.plot(title="Annual Maximum Precipitation (mm)", style="o-")
ax.set_xlabel("Year")
plt.savefig('/Users/malindgren/Documents/TEMP/test_durations.png')
plt.close()
plt.cla()

"""## 2. Carry out extreme value analysis for each duration

This is a primary step that has been shown before (part 2). For the purpose of illustration, the GUM parameters are used for further analysis.
"""

"""### Fit Gumble distribution for each duration"""

fitted_gev = OrderedDict()
for name in data.columns:
    # gevfit[name] = lmoments.pelgev(lmoments.samlmu(dx) )
    fitted_gev[name] = distr.gev(**distr.gev.lmom_fit(data=data[name]))


"""### Sample each time series"""

samples = np.linspace(data.min()[0],data.max()[4],100)

"""### Convert samples into probabilities"""

durations = ['2h', '3h', '6h', '12h', '24h',]# '2d', '3d', '4d', '7d', '10d', '20d', '30d', '45d', '60d']
samprobs = pd.DataFrame({ d:fitted_gev[d].cdf(samples) for d in durations })
samprobs.index = samples

colors = ["red","blue","cyan","green","black"]
ax = samprobs.plot(kind='line',color=colors)

"""### Compare with observations"""
def plot_ecdf(data, duration, color):
    ecdf1h = ECDF(data[duration])
    ax.plot(data[duration],ecdf1h(data[duration]),'o',c=color,label=duration)

# plot the empirical values on top
[ plot_ecdf(data,d,c) for d,c in zip(durations,colors) ]

ax.set_title('Lssp 1h')
ax.set_xlabel('Rainfall (mm)')
ax.set_ylabel('P[H<h]')

# dump tp disk
plt.savefig('/Users/malindgren/Documents/TEMP/plt_samprobs.png')
plt.close()
plt.cla()



"""## 3. Fit extreme precipitation into theorectical DDF curve for each return year

### Setup some return years (e.g., 2, 10, 20, 50, 100, 200)
"""

q2   = 1-1/2.
q10  = 1-1/10.
q20  = 1-1/20.
q50  = 1-1/50.
q100 = 1-1/100.
q200 = 1-1/200.

q = [q2,q10,q20,q50, q100, q200]

"""### Calculate extreme values for return years and durations"""

samprobs = OrderedDict()

for name in durations:
    samprobs[name] = fitted_gev[name].ppf(q)

pts = pd.DataFrame.from_dict(samprobs, orient='index')
pts.columns=[2,10,20,50, 100, 200]
pts

"""#### convert index into numbers as axis of plotting"""

pts.index = [1,3,6,12,24]
pts

pts.plot(style="o-")
# plt.xlim([0,30])
# plt.ylim([30,200])

ag = pts.plot(style="o-")
ag.set_yscale('log')
ag.set_xscale('log')
# plt.xlim([0,30])
# plt.ylim([30,200])
# plt.xlim([0.7,30])
# plt.ylim([30,200])
plt.savefig('/Users/malindgren/Documents/TEMP/plt_samprobs_2.png')
plt.close()
plt.cla()








"""### The simplest DDF curve formula is like

$$ h(t_p, Tr) = a(T_r) t_p^n$$
$$\log h(t_p, Tr) = \log a(T_r) + n \log t_p $$

### Fit DDF curves based the above formula
numpy.polyfit is used to fit the above curve for each return year in 10, 20 and 100 years.
"""

ddfparams = OrderedDict()
for name in pts.columns.values.tolist(): 
    ddfparams[name] = np.polyfit(np.log(pts[name].index),np.log(pts[name]),1)
    ddfparams[name][1] = np.exp( ddfparams[name][1])

fnl = pd.DataFrame.from_dict(ddfparams, orient='index')
fnl.columns=["n","a"]
fnl

"""### Setup some durations for interpolation and plotting (<=30 hours)"""

samdurations = np.linspace(0.1,30,100)
samdurations[-5:]

"""### Use the ddf regression parameters to calculate ddf values"""

def regddf(duration, intercepta, expn):
    return intercepta*duration**expn

fnlt = fnl.T

ddfvalues = OrderedDict()
for name in pts.columns.values.tolist():     
    ddfvalues[name] = regddf(samdurations,fnlt[name]["a"],fnlt[name]["n"])
    
inh = pd.DataFrame.from_dict(ddfvalues, orient='index').T.set_index(samdurations)

# set column names...

inh.columns = ["Tr = 2","Tr = 10","Tr = 20","Tr = 50","Tr = 100", "Tr = 200"]

"""#### Have a visualization check"""

ag = inh.plot()

ag = plt.plot(pts[2],  "o", color="blue")
ag = plt.plot(pts[10], "o", color="orange")
ag = plt.plot(pts[20], "o", color="green")
ag = plt.plot(pts[100],"o", color="purple")
ag = plt.plot(pts[200],"o", color="brown")

# plt.xlim([0.7,30])
# plt.ylim([50,100])

plt.xlabel("Duration (Hour)")
plt.ylabel("Depth of Precipitation (mm)")

ag = inh.plot()
ag.set_yscale('log')
ag.set_xscale('log')

ag = plt.plot(pts[2],  "o", color="blue")
ag = plt.plot(pts[10], "o", color="orange")
ag = plt.plot(pts[20], "o", color="green")
ag = plt.plot(pts[100],"o", color="purple")
ag = plt.plot(pts[200],"o", color="brown")

# plt.xlim([0.7,30])
# plt.ylim([50,200])

plt.xlabel("Duration (Hour) in Log Transformation")
plt.ylabel("Depth of Precipitation (mm)")

"""## End Notes

Through this guide I have tried to give you a basic idea how to carry out extreme value analysis (EVA) upon a time series data of interest.

In fact, the analysis will become more complicated in a real practice as high quality data are not always available. Sometimes, you have to spend a lot of time cleaning the data. For example, fill the missing values and check outliers.

In addition, carrying out EVA always fits several distributions, simultaneouly. Then a goodness-of-fit measure (e.g., Anderson-Darling test) is used to select the optimal one.

This is just a start. You can try more.
"""

